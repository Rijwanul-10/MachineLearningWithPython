{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation & Selection\n",
        "\n",
        "<div style=\"text-align: center; padding: 20px;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/matplotlib/matplotlib/master/doc/_static/logo2.svg\" width=\"400px\">\n",
        "    <h2>Session on ROC Curves, Cross-Validation, and Hyperparameter Tuning</h2>\n",
        "</div>\n",
        "\n",
        "## Overview of Today's Class\n",
        "\n",
        "- **ROC Curve & AUC**: Understanding how to evaluate classification models\n",
        "- **Cross-Validation**: Techniques to ensure your model generalizes well\n",
        "- **Hyperparameter Tuning**: Methods to optimize your model's performance\n",
        "\n",
        "Let's get started! ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Import the libraries we'll need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set a nice style for our plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: ROC Curve in Machine Learning\n",
        "\n",
        "## What is a ROC Curve?\n",
        "\n",
        "ROC stands for **Receiver Operating Characteristic**. It's a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
        "\n",
        "Think of it as a way to visualize the trade-off between:\n",
        "- Catching all the positive cases (sensitivity)\n",
        "- Avoiding false alarms (specificity)\n",
        "\n",
        "### Requirements for ROC Analysis:\n",
        "- Binary classification problem\n",
        "- Model that outputs probability scores/confidence levels\n",
        "- Ability to vary the classification threshold\n",
        "\n",
        "Let's start by creating some sample data to work with!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a synthetic dataset for binary classification\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, \n",
        "    n_features=10, \n",
        "    n_informative=5, \n",
        "    n_redundant=2, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "print(f\"Features: {X_train.shape[1]}\")\n",
        "print(f\"Class distribution in training set: {np.bincount(y_train)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Confusion Matrix\n",
        "\n",
        "Before we dive into ROC curves, let's understand the **confusion matrix** - the foundation of many classification metrics.\n",
        "\n",
        "A confusion matrix shows the counts of predictions vs actual values:\n",
        "\n",
        "![Confusion Matrix](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
        "\n",
        "Let's train a simple logistic regression model and see its confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Train a logistic regression model\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a more informative visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "\n",
        "# Let's annotate what each quadrant means\n",
        "plt.text(-0.3, -0.3, 'True Negative (TN)', ha='center')\n",
        "plt.text(1.7, -0.3, 'False Positive (FP)\\n(Type I Error)', ha='center')\n",
        "plt.text(-0.3, 1.7, 'False Negative (FN)\\n(Type II Error)', ha='center')\n",
        "plt.text(1.7, 1.7, 'True Positive (TP)', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate some initial metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding TPR and FPR\n",
        "\n",
        "The ROC curve plots two key metrics:\n",
        "\n",
        "### 1. True Positive Rate (TPR) aka Sensitivity or Recall\n",
        "- Formula: $TPR = \\frac{TP}{TP + FN}$\n",
        "- Interpretation: \"Of all the actual positive cases, what fraction did we correctly identify?\"\n",
        "\n",
        "### 2. False Positive Rate (FPR) aka 1-Specificity\n",
        "- Formula: $FPR = \\frac{FP}{FP + TN}$\n",
        "- Interpretation: \"Of all the actual negative cases, what fraction did we incorrectly identify as positive?\"\n",
        "\n",
        "Let's calculate these metrics for our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Get the predicted probabilities for the positive class\n",
        "y_probs = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC curve components\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate AUC (Area Under Curve)\n",
        "auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# Let's make an interactive visualization that shows what happens at different thresholds\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random chance')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.title('ROC Curve', fontsize=14)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot threshold distribution\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(y_probs, bins=20, color='green', alpha=0.6)\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', label='Default threshold (0.5)')\n",
        "plt.xlabel('Predicted Probabilities')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Predicted Probabilities', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot TPR/FPR vs threshold\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(thresholds, tpr[:-1], 'b-', label='TPR (Sensitivity)')\n",
        "plt.plot(thresholds, fpr[:-1], 'r-', label='FPR (1-Specificity)')\n",
        "plt.axvline(x=0.5, color='gray', linestyle='--', label='Default threshold (0.5)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Rate')\n",
        "plt.title('TPR and FPR at Different Thresholds', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall vs Threshold\n",
        "precision = []\n",
        "recall = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_temp = (y_probs >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_temp).ravel()\n",
        "    \n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    precision.append(prec)\n",
        "    recall.append(rec)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(thresholds, precision, 'g-', label='Precision')\n",
        "plt.plot(thresholds, recall, 'b-', label='Recall')\n",
        "plt.axvline(x=0.5, color='gray', linestyle='--', label='Default threshold (0.5)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision and Recall at Different Thresholds', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC Score: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different Cases of TPR & FPR\n",
        "\n",
        "Let's examine what different positions on the ROC curve mean:\n",
        "\n",
        "1. **Perfect Classifier**: TPR = 1, FPR = 0 (Top left corner)\n",
        "2. **Random Classifier**: Points along the diagonal line (TPR = FPR)\n",
        "3. **Better than Random**: Points above the diagonal\n",
        "4. **Worse than Random**: Points below the diagonal\n",
        "\n",
        "Let's compare different models on the same ROC plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Define different classification models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot the random chance line\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random chance')\n",
        "\n",
        "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
        "for (name, model), color in zip(models.items(), colors):\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Get predictions\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_probs = model.predict_proba(X_test)[:, 1]\n",
        "    else:  # For SVM without probability=True\n",
        "        y_probs = model.decision_function(X_test)\n",
        "        \n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "    auc = roc_auc_score(y_test, y_probs)\n",
        "    \n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {auc:.3f})')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('ROC Curves for Different Models', fontsize=16)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add annotations to highlight different regions of the plot\n",
        "plt.annotate('Better Models', xy=(0.2, 0.8), fontsize=12, \n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.2))\n",
        "plt.annotate('Perfect Classification', xy=(0.05, 0.95), fontsize=12, \n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.2))\n",
        "plt.annotate('Random Chance', xy=(0.5, 0.5), fontsize=12, \n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.2))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  Interactive Exercise: ROC Curve Analysis\n",
        "\n",
        "Let's create a function that allows us to see how changing the threshold affects the confusion matrix and metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def threshold_analysis(threshold):\n",
        "    \"\"\"Analyze model performance at a specific threshold\"\"\"\n",
        "    # Use the logistic regression model from earlier\n",
        "    y_probs = lr_model.predict_proba(X_test)[:, 1]\n",
        "    y_pred_at_threshold = (y_probs >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_at_threshold)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity/Recall\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # 1-Specificity\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
        "    \n",
        "    # Visualize results\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    \n",
        "    # Confusion matrix\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title(f'Confusion Matrix (Threshold = {threshold:.2f})', fontsize=14)\n",
        "    \n",
        "    # Highlight position on ROC curve\n",
        "    plt.subplot(2, 2, 2)\n",
        "    \n",
        "    # Calculate full ROC curve\n",
        "    full_fpr, full_tpr, all_thresholds = roc_curve(y_test, y_probs)\n",
        "    auc = roc_auc_score(y_test, y_probs)\n",
        "    \n",
        "    # Plot ROC curve\n",
        "    plt.plot(full_fpr, full_tpr, 'b-', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random chance')\n",
        "    \n",
        "    # Highlight current point\n",
        "    plt.plot(fpr, tpr, 'ro', markersize=10, label=f'Threshold = {threshold:.2f}')\n",
        "    \n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve with Selected Threshold', fontsize=14)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Metrics visualization\n",
        "    plt.subplot(2, 2, 3)\n",
        "    metrics = ['Accuracy', 'Sensitivity', '1-Specificity', 'Precision', 'F1 Score']\n",
        "    values = [accuracy, tpr, fpr, precision, f1]\n",
        "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
        "    \n",
        "    bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
        "    plt.ylim([0, 1.05])\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Performance Metrics', fontsize=14)\n",
        "    \n",
        "    # Add value labels on top of bars\n",
        "    for bar, value in zip(bars, values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, value + 0.02,\n",
        "                 f'{value:.3f}', ha='center', fontsize=10)\n",
        "    \n",
        "    # Probability distribution\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.histplot(y_probs, bins=30, kde=True, color='skyblue')\n",
        "    plt.axvline(x=threshold, color='red', linestyle='--', \n",
        "                label=f'Threshold = {threshold:.2f}')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Probability Distribution', fontsize=14)\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Performance at threshold {threshold:.2f}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sensitivity (TPR): {tpr:.4f}\")\n",
        "    print(f\"Specificity: {1-fpr:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Let's see what happens at the default threshold of 0.5\n",
        "threshold_analysis(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Try with a lower threshold - will increase sensitivity but reduce specificity\n",
        "threshold_analysis(0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Try with a higher threshold - will increase specificity but reduce sensitivity\n",
        "threshold_analysis(0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use ROC AUC\n",
        "\n",
        "ROC AUC is excellent for:\n",
        "- Comparing different classification models\n",
        "- Evaluating performance across all thresholds\n",
        "- Problems where you need to balance sensitivity and specificity\n",
        "\n",
        "**Key considerations:**\n",
        "- Best for balanced datasets\n",
        "- For imbalanced data, precision-recall curves may be better\n",
        "- Appropriate when both false positives and false negatives have similar costs\n",
        "\n",
        "Let's try an example with a real-world dataset - breast cancer diagnosis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "# Split the data\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot the random chance line\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random chance')\n",
        "\n",
        "colors = ['blue', 'red', 'green']\n",
        "for (name, model), color in zip(models.items(), colors):\n",
        "    model.fit(X_train_c, y_train_c)\n",
        "    y_probs = model.predict_proba(X_test_c)[:, 1]\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(y_test_c, y_probs)\n",
        "    auc = roc_auc_score(y_test_c, y_probs)\n",
        "    \n",
        "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {auc:.3f})')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
        "plt.title('ROC Curves for Breast Cancer Diagnosis', fontsize=16)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add context about the application\n",
        "plt.annotate('Lower threshold = More patients get treatment\\n(higher sensitivity, more false positives)', \n",
        "             xy=(0.3, 0.7), fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.2))\n",
        "plt.annotate('Higher threshold = Fewer patients get treatment\\n(lower sensitivity, fewer false positives)', \n",
        "             xy=(0.7, 0.3), fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.2))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show class distribution\n",
        "print(\"Class distribution in the breast cancer dataset:\")\n",
        "print(f\"Benign (0): {sum(y_cancer == 0)}\")\n",
        "print(f\"Malignant (1): {sum(y_cancer == 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Cross-Validation\n",
        "\n",
        "## Why Do We Need Cross-Validation?\n",
        "\n",
        "Cross-validation helps us:\n",
        "1. **Validate model generalizability**: Ensure our model performs well on unseen data\n",
        "2. **Avoid overfitting**: Detect if the model is memorizing training data\n",
        "3. **Make better use of limited data**: Use all data for both training and validation\n",
        "4. **Get more reliable performance estimates**: Reduce variance in performance metrics\n",
        "\n",
        "Let's visualize the problem of simple train/test splitting versus cross-validation:"
      ]
    },
    {
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Visualizing cross-validation vs. single train/test split\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a small dataset with some noise for demonstration\n",
        "n_samples = 100\n",
        "X_demo = np.random.randn(n_samples, 1) * 3\n",
        "y_demo = (X_demo[:, 0] > 0).astype(int) + np.random.binomial(1, 0.2, size=n_samples)  # Add noise\n",
        "y_demo = y_demo % 2  # Make it binary again\n",
        "\n",
        "# Function to create multiple train/test splits and evaluate\n",
        "def evaluate_multiple_splits(X, y, n_splits=5, test_size=0.3):\n",
        "    accuracies = []\n",
        "    models = []\n",
        "    all_X_train = []\n",
        "    all_X_test = []\n",
        "    all_y_train = []\n",
        "    all_y_test = []\n",
        "    \n",
        "    for i in range(n_splits):\n",
        "        # Create different random splits\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=i\n",
        "        )\n",
        "        \n",
        "        all_X_train.append(X_train)\n",
        "        all_X_test.append(X_test)\n",
        "        all_y_train.append(y_train)\n",
        "        all_y_test.append(y_test)\n",
        "        \n",
        "        # Train a model\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "        models.append(model)\n",
        "        \n",
        "        # Evaluate\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        accuracies.append(acc)\n",
        "    \n",
        "    return accuracies, models, all_X_train, all_X_test, all_y_train, all_y_test\n",
        "\n",
        "# Run evaluation\n",
        "accs, models, X_trains, X_tests, y_trains, y_tests = evaluate_multiple_splits(X_demo, y_demo)\n",
        "\n",
        "# Visualize different splits and their decision boundaries\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "for i in range(5):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    \n",
        "    # Plot training data\n",
        "    plt.scatter(X_trains[i][y_trains[i]==0, 0], [0.1]*sum(y_trains[i]==0), \n",
        "                c='blue', marker='o', alpha=0.5, label='Train: Class 0')\n",
        "    plt.scatter(X_trains[i][y_trains[i]==1, 0], [0.1]*sum(y_trains[i]==1), \n",
        "                c='red', marker='o', alpha=0.5, label='Train: Class 1')\n",
        "    \n",
        "    # Plot test data\n",
        "    plt.scatter(X_tests[i][y_tests[i]==0, 0], [0.9]*sum(y_tests[i]==0), \n",
        "                c='blue', marker='x', alpha=0.7, label='Test: Class 0')\n",
        "    plt.scatter(X_tests[i][y_tests[i]==1, 0], [0.9]*sum(y_tests[i]==1), \n",
        "                c='red', marker='x', alpha=0.7, label='Test: Class 1')\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    x_min, x_max = X_demo[:, 0].min() - 1, X_demo[:, 0].max() + 1\n",
        "    xx = np.linspace(x_min, x_max, 100).reshape(-1, 1)\n",
        "    yy = models[i].predict_proba(xx)[:, 1]\n",
        "    threshold = 0.5\n",
        "    boundary_x = xx[np.abs(yy - threshold).argmin()][0]\n",
        "    plt.axvline(x=boundary_x, color='purple', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    plt.yticks([0.1, 0.9], ['Train', 'Test'])\n",
        "    plt.title(f'Split {i+1}: Accuracy = {accs[i]:.2f}')\n",
        "    plt.xlabel('Feature Value')\n",
        "    if i == 0:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "# Add a summary subplot\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.boxplot(accs)\n",
        "plt.title(f'Accuracy Distribution\\nMean: {np.mean(accs):.2f}, Std: {np.std(accs):.2f}')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks([1], ['Different Train/Test Splits'])\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how the decision boundary and accuracy change with different random splits!\")\n",
        "print(f\"Accuracy range: {min(accs):.2f} - {max(accs):.2f}\")\n",
        "print(f\"Standard deviation: {np.std(accs):.4f}\")\n",
        "print(\"\\nThis is why we need cross-validation - to get a more reliable estimate of model performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Validation Methods\n",
        "\n",
        "Let's explore three common cross-validation techniques:\n",
        "\n",
        "1. **Leave-One-Out Cross-Validation (LOOCV)**\n",
        "2. **K-Fold Cross-Validation**\n",
        "3. **Stratified K-Fold Cross-Validation**\n",
        "\n",
        "Let's visualize each method to understand them better:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create a simple dataset for visualization\n",
        "np.random.seed(42)\n",
        "n = 20  # Small dataset for clear visualization\n",
        "X_small = np.random.rand(n, 2)\n",
        "y_small = np.random.choice([0, 1], size=n)\n",
        "\n",
        "# Add some structure to the data\n",
        "y_small[(X_small[:, 0] > 0.5) & (X_small[:, 1] > 0.5)] = 1\n",
        "y_small[(X_small[:, 0] < 0.5) & (X_small[:, 1] < 0.5)] = 0\n",
        "\n",
        "# Helper function to visualize CV splits\n",
        "def plot_cv_splits(cv, X, y, ax, title):\n",
        "    cmap = plt.cm.tab10\n",
        "    for i, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
        "        # Plot all samples with alpha transparency\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, alpha=0.1, \n",
        "                  s=100, edgecolors='k')\n",
        "        \n",
        "        # Highlight test samples for this split\n",
        "        test_points = X[test_index]\n",
        "        test_labels = y[test_index]\n",
        "        ax.scatter(test_points[:, 0], test_points[:, 1], \n",
        "                  c=test_labels, cmap=plt.cm.coolwarm, alpha=1.0,\n",
        "                  s=100, edgecolors='k')\n",
        "        \n",
        "        # Draw a boundary around test points\n",
        "        for j in test_index:\n",
        "            ax.annotate(f'Fold {i+1}', (X[j, 0], X[j, 1]), \n",
        "                       xytext=(10, 5), textcoords='offset points',\n",
        "                       fontsize=8, color=cmap(i % 10))\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.set_xlim(-0.1, 1.1)\n",
        "    ax.set_ylim(-0.1, 1.1)\n",
        "\n",
        "# Create a figure for CV comparison\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Create different CV splitters\n",
        "cv_loocv = LeaveOneOut()\n",
        "cv_kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Plot each CV strategy\n",
        "ax1 = plt.subplot(1, 3, 1)\n",
        "plot_cv_splits(cv_loocv, X_small, y_small, ax1, 'Leave-One-Out Cross-Validation')\n",
        "\n",
        "ax2 = plt.subplot(1, 3, 2)\n",
        "plot_cv_splits(cv_kfold, X_small, y_small, ax2, 'K-Fold Cross-Validation (k=5)')\n",
        "\n",
        "ax3 = plt.subplot(1, 3, 3)\n",
        "plot_cv_splits(cv_stratified, X_small, y_small, ax3, 'Stratified K-Fold Cross-Validation (k=5)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leave-One-Out Cross-Validation (LOOCV)\n",
        "\n",
        "In LOOCV, we:\n",
        "- Use 1 observation for testing and the rest for training\n",
        "- Repeat this process for all observations\n",
        "- Average the results\n",
        "\n",
        "### Advantages:\n",
        "- Uses almost all data for training\n",
        "- No random sampling, so results are deterministic\n",
        "- Minimal bias\n",
        "\n",
        "### Disadvantages:\n",
        "- Computationally expensive (n models for n samples)\n",
        "- High variance in test results\n",
        "- Can be impractical for large datasets\n",
        "\n",
        "### When to use:\n",
        "- Small datasets where you need to maximize training data\n",
        "- When computational resources aren't a concern\n",
        "- When you need deterministic results\n",
        "\n",
        "Let's implement LOOCV on our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Implement LOOCV on our classification dataset\n",
        "loocv = LeaveOneOut()\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# We'll use a smaller subset of data since LOOCV is computationally intensive\n",
        "X_subset = X[:100]\n",
        "y_subset = y[:100]\n",
        "\n",
        "loocv_scores = []\n",
        "\n",
        "# Execute LOOCV\n",
        "for train_idx, test_idx in loocv.split(X_subset):\n",
        "    X_train, X_test = X_subset[train_idx], X_subset[test_idx]\n",
        "    y_train, y_test = y_subset[train_idx], y_subset[test_idx]\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    loocv_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(loocv_scores, bins=2, alpha=0.7, color='blue')\n",
        "plt.axvline(x=np.mean(loocv_scores), color='red', linestyle='--', \n",
        "            label=f'Mean: {np.mean(loocv_scores):.3f}')\n",
        "plt.title('Leave-One-Out Cross-Validation Results', fontsize=14)\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"LOOCV Mean Accuracy: {np.mean(loocv_scores):.4f}\")\n",
        "print(f\"LOOCV Standard Deviation: {np.std(loocv_scores):.4f}\")\n",
        "print(f\"LOOCV Range: {min(loocv_scores):.4f} - {max(loocv_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Fold Cross-Validation\n",
        "\n",
        "In K-Fold CV, we:\n",
        "- Split data into k equally sized folds\n",
        "- Use 1 fold for testing and k-1 folds for training\n",
        "- Repeat this process k times, using each fold as the test set once\n",
        "- Average the results\n",
        "\n",
        "### Advantages:\n",
        "- Less computationally expensive than LOOCV\n",
        "- Lower variance than LOOCV\n",
        "- Works well for medium to large datasets\n",
        "\n",
        "### Disadvantages:\n",
        "- May not preserve class distribution in each fold\n",
        "- Results can vary with different random splits\n",
        "- Uses less training data than LOOCV\n",
        "\n",
        "### When to use:\n",
        "- Medium to large datasets\n",
        "- When computational resources are limited\n",
        "- When you need a balance between bias and variance\n",
        "\n",
        "Let's implement K-Fold CV:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Implement K-Fold CV\n",
        "k_values = [3, 5, 10]\n",
        "results = []\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, k in enumerate(k_values):\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    \n",
        "    # Get scores for each fold\n",
        "    fold_scores = cross_val_score(model, X, y, cv=kfold)\n",
        "    results.append(fold_scores)\n",
        "    \n",
        "    # Plot individual fold results\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    bars = plt.bar(range(1, k+1), fold_scores, alpha=0.7)\n",
        "    plt.axhline(y=np.mean(fold_scores), color='red', linestyle='--', \n",
        "                label=f'Mean: {np.mean(fold_scores):.3f}')\n",
        "    \n",
        "    # Add value labels on top of bars\n",
        "    for bar, score in zip(bars, fold_scores):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, score + 0.01,\n",
        "                 f'{score:.3f}', ha='center', fontsize=9)\n",
        "    \n",
        "    plt.title(f'{k}-Fold Cross-Validation Results', fontsize=14)\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0.65, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot comparison of different k values\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.boxplot(results)\n",
        "plt.title('Comparison of Different K Values', fontsize=14)\n",
        "plt.xlabel('Number of Folds (K)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks([1, 2, 3], k_values)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "for k, scores in zip(k_values, results):\n",
        "    print(f\"{k}-Fold CV Mean Accuracy: {np.mean(scores):.4f}\")\n",
        "    print(f\"{k}-Fold CV Standard Deviation: {np.std(scores):.4f}\")\n",
        "    print(f\"{k}-Fold CV Range: {min(scores):.4f} - {max(scores):.4f}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stratified K-Fold Cross-Validation\n",
        "\n",
        "Stratified K-Fold CV is a variation of K-Fold CV that preserves the class distribution in each fold. This is especially important for imbalanced datasets.\n",
        "\n",
        "### How it works:\n",
        "- Similar to K-Fold CV, but maintains the same class proportions in each fold\n",
        "- Ensures that each fold is representative of the overall class distribution\n",
        "\n",
        "### Advantages:\n",
        "- Better for imbalanced datasets\n",
        "- More reliable performance estimates\n",
        "- Reduces bias in the evaluation\n",
        "\n",
        "### Disadvantages:\n",
        "- Slightly more complex implementation\n",
        "- May not be necessary for balanced datasets\n",
        "\n",
        "### When to use:\n",
        "- Imbalanced datasets\n",
        "- When class distribution is important\n",
        "- When you want more reliable performance estimates\n",
        "\n",
        "Let's compare regular K-Fold CV with Stratified K-Fold CV on an imbalanced dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create an imbalanced dataset\n",
        "np.random.seed(42)\n",
        "X_imb = np.random.randn(1000, 5)\n",
        "y_imb = np.zeros(1000)\n",
        "# Make only 10% of samples positive\n",
        "y_imb[:100] = 1\n",
        "\n",
        "# Shuffle the data\n",
        "indices = np.random.permutation(len(X_imb))\n",
        "X_imb, y_imb = X_imb[indices], y_imb[indices]\n",
        "\n",
        "# Set up CV strategies\n",
        "k = 5\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Compare class distribution in each fold\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Regular K-Fold\n",
        "plt.subplot(2, 1, 1)\n",
        "fold_pos_counts = []\n",
        "for i, (_, test_idx) in enumerate(kfold.split(X_imb)):\n",
        "    y_test_fold = y_imb[test_idx]\n",
        "    pos_count = sum(y_test_fold == 1)\n",
        "    neg_count = sum(y_test_fold == 0)\n",
        "    fold_pos_counts.append(pos_count)\n",
        "    \n",
        "    plt.bar(i, pos_count / len(test_idx) * 100, color='red', alpha=0.7, label='Positive' if i == 0 else \"\")\n",
        "    plt.bar(i, neg_count / len(test_idx) * 100, bottom=pos_count / len(test_idx) * 100, \n",
        "            color='blue', alpha=0.7, label='Negative' if i == 0 else \"\")\n",
        "\n",
        "plt.axhline(y=10, color='red', linestyle='--', label='Overall positive rate (10%)')\n",
        "plt.title('Class Distribution in Regular K-Fold CV', fontsize=14)\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(range(k), [f'Fold {i+1}' for i in range(k)])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Stratified K-Fold\n",
        "plt.subplot(2, 1, 2)\n",
        "strat_fold_pos_counts = []\n",
        "for i, (_, test_idx) in enumerate(stratified_kfold.split(X_imb, y_imb)):\n",
        "    y_test_fold = y_imb[test_idx]\n",
        "    pos_count = sum(y_test_fold == 1)\n",
        "    neg_count = sum(y_test_fold == 0)\n",
        "    strat_fold_pos_counts.append(pos_count)\n",
        "    \n",
        "    plt.bar(i, pos_count / len(test_idx) * 100, color='red', alpha=0.7, label='Positive' if i == 0 else \"\")\n",
        "    plt.bar(i, neg_count / len(test_idx) * 100, bottom=pos_count / len(test_idx) * 100, \n",
        "            color='blue', alpha=0.7, label='Negative' if i == 0 else \"\")\n",
        "\n",
        "plt.axhline(y=10, color='red', linestyle='--', label='Overall positive rate (10%)')\n",
        "plt.title('Class Distribution in Stratified K-Fold CV', fontsize=14)\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(range(k), [f'Fold {i+1}' for i in range(k)])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare performance\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "reg_scores = cross_val_score(model, X_imb, y_imb, cv=kfold)\n",
        "strat_scores = cross_val_score(model, X_imb, y_imb, cv=stratified_kfold)\n",
        "\n",
        "print(\"Regular K-Fold CV Results:\")\n",
        "print(f\"Mean Accuracy: {np.mean(reg_scores):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(reg_scores):.4f}\")\n",
        "print(\"---\")\n",
        "print(\"Stratified K-Fold CV Results:\")\n",
        "print(f\"Mean Accuracy: {np.mean(strat_scores):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(strat_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  Interactive Exercise: Cross-Validation Comparison\n",
        "\n",
        "Let's create a function that allows us to compare different cross-validation methods on various models:"
      ]
    },
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}